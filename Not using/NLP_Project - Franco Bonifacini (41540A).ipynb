{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2ffbc47",
   "metadata": {},
   "source": [
    "# Politics of Emotions or Propaganda?\n",
    "_Franco Reinaldo Bonifacini (41540A)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef930e3",
   "metadata": {},
   "source": [
    "## Libraries (Requirements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9cfa5cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from -r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from -r requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from -r requirements.txt (line 3)) (3.10.3)\n",
      "Requirement already satisfied: seaborn in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from -r requirements.txt (line 4)) (0.13.2)\n",
      "Requirement already satisfied: torch in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from -r requirements.txt (line 5)) (2.7.1)\n",
      "Requirement already satisfied: Transformers in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from -r requirements.txt (line 6)) (4.52.4)\n",
      "Requirement already satisfied: nltk in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from -r requirements.txt (line 7)) (3.9.1)\n",
      "Requirement already satisfied: pipeline in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from -r requirements.txt (line 8)) (0.1.0)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from -r requirements.txt (line 10)) (0.1.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from -r requirements.txt (line 11)) (4.67.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from -r requirements.txt (line 12)) (1.5.1)\n",
      "Requirement already satisfied: huggingface_hub[hf_xet] in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from -r requirements.txt (line 9)) (0.33.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (3.2.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from torch->-r requirements.txt (line 5)) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from torch->-r requirements.txt (line 5)) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from torch->-r requirements.txt (line 5)) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from torch->-r requirements.txt (line 5)) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from torch->-r requirements.txt (line 5)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from torch->-r requirements.txt (line 5)) (2025.5.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from torch->-r requirements.txt (line 5)) (80.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from Transformers->-r requirements.txt (line 6)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from Transformers->-r requirements.txt (line 6)) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from Transformers->-r requirements.txt (line 6)) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from Transformers->-r requirements.txt (line 6)) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from Transformers->-r requirements.txt (line 6)) (0.5.3)\n",
      "Requirement already satisfied: click in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from nltk->-r requirements.txt (line 7)) (8.2.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from huggingface_hub[hf_xet]->-r requirements.txt (line 9)) (1.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from tqdm->-r requirements.txt (line 11)) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from jinja2->torch->-r requirements.txt (line 5)) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from requests->Transformers->-r requirements.txt (line 6)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from requests->Transformers->-r requirements.txt (line 6)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from requests->Transformers->-r requirements.txt (line 6)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\franc\\documents\\github\\nlp_project\\.venv\\lib\\site-packages (from requests->Transformers->-r requirements.txt (line 6)) (2025.6.15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3844095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b8b4dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: False\n",
      "MPS: False\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "print(f\"MPS: {torch.mps.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bc9bc9",
   "metadata": {},
   "source": [
    "As my computer does not have GPU NVIDIA, I can't implement CUDA, so I'll use PyTorch library but will run slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6198ba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if there is a GPU (CUDA), if not use CPU (I already know that in my case I'll use CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b0cd73",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "The dataset was previously scraped from the official website of the argentinian presidency (https://www.casarosada.gob.ar/informacion/discursos), automatically clean through the use of \"Raw Data Manipulation\" code, and finally manually cleaned as there are particular cases that couldn't been cleaned with previous code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcc683ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speeches = pd.read_excel(r\"C:\\Users\\franc\\Documents\\GitHub\\nlp_project\\df_final.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a12785d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1274 entries, 0 to 1273\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   date             1274 non-null   datetime64[ns]\n",
      " 1   president        1274 non-null   object        \n",
      " 2   content          1274 non-null   object        \n",
      " 3   cleaned_content  1274 non-null   object        \n",
      "dtypes: datetime64[ns](1), object(3)\n",
      "memory usage: 39.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_speeches.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "360e13dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>president</th>\n",
       "      <th>content</th>\n",
       "      <th>cleaned_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-17</td>\n",
       "      <td>Javier Milei</td>\n",
       "      <td>Buenas tardes, muchas gracias: hoy estoy acá p...</td>\n",
       "      <td>Buenas tardes, muchas gracias: hoy estoy acá p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-04-02</td>\n",
       "      <td>Javier Milei</td>\n",
       "      <td>Hoy estamos aquí reunidos a 42 años del inicio...</td>\n",
       "      <td>Hoy estamos aquí reunidos a 42 años del inicio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-02-24</td>\n",
       "      <td>Javier Milei</td>\n",
       "      <td>Hola a todos. Yo soy el león. Yo también los a...</td>\n",
       "      <td>Hola a todos. Yo soy el león. Yo también los a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-26</td>\n",
       "      <td>Javier Milei</td>\n",
       "      <td>En primer lugar, quiero comenzar por agradecer...</td>\n",
       "      <td>En primer lugar, quiero comenzar por agradecer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-02-08</td>\n",
       "      <td>Javier Milei</td>\n",
       "      <td>Buenos días. Quiero compartir con ustedes una ...</td>\n",
       "      <td>Buenos días. Quiero compartir con ustedes una ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1269</th>\n",
       "      <td>2023-12-08</td>\n",
       "      <td>Alberto Fernández</td>\n",
       "      <td>Querido Pueblo Argentino: Hace exactamente 40 ...</td>\n",
       "      <td>Querido Pueblo Argentino: Hace exactamente 40 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>2023-12-10</td>\n",
       "      <td>Javier Milei</td>\n",
       "      <td>Hola a todos. Señores ministros de la Corte, s...</td>\n",
       "      <td>Hola a todos. Señores ministros de la Corte, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271</th>\n",
       "      <td>2023-12-10</td>\n",
       "      <td>Javier Milei</td>\n",
       "      <td>Hola a todos. ¡Viva la libertad, carajo! ¡Viva...</td>\n",
       "      <td>Hola a todos. ¡Viva la libertad, carajo! ¡Viva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Javier Milei</td>\n",
       "      <td>Argentinos, hoy es un día histórico para nuest...</td>\n",
       "      <td>Argentinos, hoy es un día histórico para nuest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>2023-12-30</td>\n",
       "      <td>Javier Milei</td>\n",
       "      <td>A poco de comenzar los festejos de Año Nuevo q...</td>\n",
       "      <td>A poco de comenzar los festejos de Año Nuevo q...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1274 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date          president  \\\n",
       "0    2024-01-17       Javier Milei   \n",
       "1    2024-04-02       Javier Milei   \n",
       "2    2024-02-24       Javier Milei   \n",
       "3    2024-01-26       Javier Milei   \n",
       "4    2024-02-08       Javier Milei   \n",
       "...         ...                ...   \n",
       "1269 2023-12-08  Alberto Fernández   \n",
       "1270 2023-12-10       Javier Milei   \n",
       "1271 2023-12-10       Javier Milei   \n",
       "1272 2023-12-20       Javier Milei   \n",
       "1273 2023-12-30       Javier Milei   \n",
       "\n",
       "                                                content  \\\n",
       "0     Buenas tardes, muchas gracias: hoy estoy acá p...   \n",
       "1     Hoy estamos aquí reunidos a 42 años del inicio...   \n",
       "2     Hola a todos. Yo soy el león. Yo también los a...   \n",
       "3     En primer lugar, quiero comenzar por agradecer...   \n",
       "4     Buenos días. Quiero compartir con ustedes una ...   \n",
       "...                                                 ...   \n",
       "1269  Querido Pueblo Argentino: Hace exactamente 40 ...   \n",
       "1270  Hola a todos. Señores ministros de la Corte, s...   \n",
       "1271  Hola a todos. ¡Viva la libertad, carajo! ¡Viva...   \n",
       "1272  Argentinos, hoy es un día histórico para nuest...   \n",
       "1273  A poco de comenzar los festejos de Año Nuevo q...   \n",
       "\n",
       "                                        cleaned_content  \n",
       "0     Buenas tardes, muchas gracias: hoy estoy acá p...  \n",
       "1     Hoy estamos aquí reunidos a 42 años del inicio...  \n",
       "2     Hola a todos. Yo soy el león. Yo también los a...  \n",
       "3     En primer lugar, quiero comenzar por agradecer...  \n",
       "4     Buenos días. Quiero compartir con ustedes una ...  \n",
       "...                                                 ...  \n",
       "1269  Querido Pueblo Argentino: Hace exactamente 40 ...  \n",
       "1270  Hola a todos. Señores ministros de la Corte, s...  \n",
       "1271  Hola a todos. ¡Viva la libertad, carajo! ¡Viva...  \n",
       "1272  Argentinos, hoy es un día histórico para nuest...  \n",
       "1273  A poco de comenzar los festejos de Año Nuevo q...  \n",
       "\n",
       "[1274 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005b41bf",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2f5de8",
   "metadata": {},
   "source": [
    "# Split Speeches into Sentences\n",
    "\n",
    "Split speeches into sentences (with a threshold of 50 words) so models can be used efficiently and avoid truncation of speeches when implementing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33015119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\franc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4961e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_speeches(text, max_words=50, min_words=5):\n",
    "    \"\"\"\n",
    "    Splits the text into sentences using NLTK, and further splits long sentences into word-based chunks.\n",
    "    Sentences with fewer than `min_words` are merged with the previous chunk if possible.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The full input text.\n",
    "        max_words (int): Maximum number of words per chunk.\n",
    "        min_words (int): Minimum number of words per chunk. Shorter ones are merged with the previous.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of cleaned text chunks.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = sentence.strip().split()\n",
    "\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        # Split long sentences\n",
    "        while len(words) > max_words:\n",
    "            chunks.append(\" \".join(words[:max_words]))\n",
    "            words = words[max_words:]\n",
    "\n",
    "        # Remaining words\n",
    "        leftover = \" \".join(words)\n",
    "\n",
    "        if len(words) < min_words:\n",
    "            if chunks:\n",
    "                # Merge with previous chunk\n",
    "                chunks[-1] = chunks[-1] + \" \" + leftover\n",
    "            else:\n",
    "                # If it's the first sentence, just keep it\n",
    "                chunks.append(leftover)\n",
    "        else:\n",
    "            chunks.append(leftover)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "450708fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speeches['sentences'] = df_speeches['cleaned_content'].apply(split_speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a5c189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_speeches = df_speeches.explode('sentences')[['date', 'president', 'sentences']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b977b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>president</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-17</td>\n",
       "      <td>Javier Milei</td>\n",
       "      <td>Buenas tardes, muchas gracias: hoy estoy acá p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-17</td>\n",
       "      <td>Javier Milei</td>\n",
       "      <td>Lamentablemente en las últimas décadas, motiva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-17</td>\n",
       "      <td>Javier Milei</td>\n",
       "      <td>Nosotros estamos, acá, para decirles que los e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-17</td>\n",
       "      <td>Javier Milei</td>\n",
       "      <td>Créanme, nadie mejor que nosotros los argentin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-17</td>\n",
       "      <td>Javier Milei</td>\n",
       "      <td>Cuando adoptamos el modelo de la libertad – al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51644</th>\n",
       "      <td>2023-12-30</td>\n",
       "      <td>Javier Milei</td>\n",
       "      <td>Para finalizar, quiero una vez más, desearles ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51645</th>\n",
       "      <td>2023-12-30</td>\n",
       "      <td>Javier Milei</td>\n",
       "      <td>Espero que puedan pasarlo en compañía de su fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51646</th>\n",
       "      <td>2023-12-30</td>\n",
       "      <td>Javier Milei</td>\n",
       "      <td>Este puede ser el año en que demos vuelta un s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51647</th>\n",
       "      <td>2023-12-30</td>\n",
       "      <td>Javier Milei</td>\n",
       "      <td>Mi deseo - para este Nuevo Año - es que la dir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51648</th>\n",
       "      <td>2023-12-30</td>\n",
       "      <td>Javier Milei</td>\n",
       "      <td>Por último, que Dios bendiga a los argentinos ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51649 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date     president  \\\n",
       "0     2024-01-17  Javier Milei   \n",
       "1     2024-01-17  Javier Milei   \n",
       "2     2024-01-17  Javier Milei   \n",
       "3     2024-01-17  Javier Milei   \n",
       "4     2024-01-17  Javier Milei   \n",
       "...          ...           ...   \n",
       "51644 2023-12-30  Javier Milei   \n",
       "51645 2023-12-30  Javier Milei   \n",
       "51646 2023-12-30  Javier Milei   \n",
       "51647 2023-12-30  Javier Milei   \n",
       "51648 2023-12-30  Javier Milei   \n",
       "\n",
       "                                               sentences  \n",
       "0      Buenas tardes, muchas gracias: hoy estoy acá p...  \n",
       "1      Lamentablemente en las últimas décadas, motiva...  \n",
       "2      Nosotros estamos, acá, para decirles que los e...  \n",
       "3      Créanme, nadie mejor que nosotros los argentin...  \n",
       "4      Cuando adoptamos el modelo de la libertad – al...  \n",
       "...                                                  ...  \n",
       "51644  Para finalizar, quiero una vez más, desearles ...  \n",
       "51645  Espero que puedan pasarlo en compañía de su fa...  \n",
       "51646  Este puede ser el año en que demos vuelta un s...  \n",
       "51647  Mi deseo - para este Nuevo Año - es que la dir...  \n",
       "51648  Por último, que Dios bendiga a los argentinos ...  \n",
       "\n",
       "[51649 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_split_speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7681b9",
   "metadata": {},
   "source": [
    "## Translate Sentences into English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c88b6f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "241eadc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model to translate from spanish to english, so it can be implemented correctly to the final model\n",
    "\n",
    "language_model_name = \"Helsinki-NLP/opus-mt-es-en\"\n",
    "language_tokenizer = MarianTokenizer.from_pretrained(language_model_name)\n",
    "language_model = MarianMTModel.from_pretrained(language_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b0a71a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(texts):\n",
    "    \"\"\"\n",
    "    Translates a list of texts from a source language to a target language using a transformer-based translation model.\n",
    "\n",
    "    Parameters:\n",
    "        texts (list of str): List of input text strings to be translated.\n",
    "\n",
    "    Returns:\n",
    "        list of str: List of translated text strings corresponding to each input text, in the same order.\n",
    "    \"\"\"\n",
    "\n",
    "    language_model.to(device)\n",
    "\n",
    "    inputs = language_tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Move tokenized tensors to GPU\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    translated = language_model.generate(**inputs)\n",
    "    return [language_tokenizer.decode(t, skip_special_tokens=True) for t in translated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7dd83804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_translate(df, text_column, batch_size=100, n_jobs=None):\n",
    "    \"\"\"\n",
    "    Translates texts in the `text_column` of the DataFrame `df` in batches,\n",
    "    processing batches in parallel.\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): DataFrame containing the texts.\n",
    "        text_column (str): Name of the column with the texts to translate.\n",
    "        batch_size (int): Number of texts per batch for translation.\n",
    "        n_jobs (int): Number of parallel jobs (default: half of CPU cores).\n",
    "\n",
    "    Returns:\n",
    "        list: List with all translated texts.\n",
    "    \"\"\"\n",
    "\n",
    "    texts = df[text_column].tolist()\n",
    "    batches = [texts[i:i + batch_size] for i in range(0, len(texts), batch_size)]\n",
    "\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(translate)(batch) for batch in tqdm(batches, desc=\"Translating batches\")\n",
    "    )\n",
    "\n",
    "    # Aplanar lista de listas en una sola lista\n",
    "    all_translations = [item for sublist in results for item in sublist]\n",
    "\n",
    "    return all_translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3c75e460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating batches:  15%|█▌        | 8/52 [00:17<00:00, 57.88it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m translated_texts = \u001b[43mbatch_translate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_split_speeches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_column\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msentences\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mbatch_translate\u001b[39m\u001b[34m(df, text_column, batch_size, n_jobs)\u001b[39m\n\u001b[32m     16\u001b[39m texts = df[text_column].tolist()\n\u001b[32m     17\u001b[39m batches = [texts[i:i + batch_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(texts), batch_size)]\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m results = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranslate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTranslating batches\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Aplanar lista de listas en una sola lista\u001b[39;00m\n\u001b[32m     24\u001b[39m all_translations = [item \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sublist]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\franc\\Documents\\GitHub\\nlp_project\\.venv\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\franc\\Documents\\GitHub\\nlp_project\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\franc\\Documents\\GitHub\\nlp_project\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "translated_texts = batch_translate(df_split_speeches, text_column='sentences', batch_size=1000, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a221c1cd",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31711654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56f8516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables containing the model and the tokenizer\n",
    "model_name = \"SamLowe/roberta-base-go_emotions\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, attn_implementation=\"eager\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "438905cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Creation of the emotion classification pipepline\n",
    "emotion_classifier = pipeline(\n",
    "    task=\"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    top_k=None,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe6c7c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'anger', 'score': 0.6907389163970947},\n",
       " {'label': 'neutral', 'score': 0.14974887669086456},\n",
       " {'label': 'annoyance', 'score': 0.10762118548154831},\n",
       " {'label': 'sadness', 'score': 0.019939061254262924},\n",
       " {'label': 'joy', 'score': 0.01638145186007023},\n",
       " {'label': 'disgust', 'score': 0.01580151915550232},\n",
       " {'label': 'disapproval', 'score': 0.010370961390435696},\n",
       " {'label': 'disappointment', 'score': 0.009592698886990547},\n",
       " {'label': 'approval', 'score': 0.008287952281534672},\n",
       " {'label': 'excitement', 'score': 0.006921317894011736},\n",
       " {'label': 'fear', 'score': 0.006091337651014328},\n",
       " {'label': 'admiration', 'score': 0.005420122295618057},\n",
       " {'label': 'caring', 'score': 0.005269885994493961},\n",
       " {'label': 'amusement', 'score': 0.0051732794381678104},\n",
       " {'label': 'realization', 'score': 0.0050010597333312035},\n",
       " {'label': 'love', 'score': 0.004166232421994209},\n",
       " {'label': 'surprise', 'score': 0.0035567530430853367},\n",
       " {'label': 'grief', 'score': 0.002567428397014737},\n",
       " {'label': 'desire', 'score': 0.002419997239485383},\n",
       " {'label': 'optimism', 'score': 0.0022826988715678453},\n",
       " {'label': 'curiosity', 'score': 0.0022512045688927174},\n",
       " {'label': 'confusion', 'score': 0.0021561237517744303},\n",
       " {'label': 'embarrassment', 'score': 0.0020815033931285143},\n",
       " {'label': 'pride', 'score': 0.001919951755553484},\n",
       " {'label': 'nervousness', 'score': 0.0014304454671218991},\n",
       " {'label': 'relief', 'score': 0.001075738575309515},\n",
       " {'label': 'gratitude', 'score': 0.0007503990200348198},\n",
       " {'label': 'remorse', 'score': 0.0006125715444795787}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try a sentence outputed by the translation model\n",
    "example = ['Long live the fucking freedom.']\n",
    "\n",
    "model_output = emotion_classifier(example)\n",
    "model_output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cca4de",
   "metadata": {},
   "source": [
    "With this two models created (translation and classifier) I will translate and classify the text, and return the label and score of the most probable emotion included in the given speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b8b5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_emotion(text, classifier):\n",
    "    results = classifier(text)[0]\n",
    "    top = max(results, key=lambda x: x['score'])\n",
    "    return pd.Series([top['label'], top['score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fb242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_speeches[['emotion', 'score']] = df_speeches['content'].apply(get_top_emotion_row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
